{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet Inference (Old & New Data)\n",
    "\n",
    "---\n",
    "\n",
    "This is an inference Kernel. To see how I got the model, please go to the [Training Kernel here](https://www.kaggle.com/raimonds1993/aptos19-densenet-trained-with-old-and-new-data). \n",
    "\n",
    "If this helped you, <span style=\"color:red\"> please upvote </span>.\n",
    "\n",
    "As a recap, I trained a Denset on both old (2015) and new (2019) competition dataset. Considering the size of the overall dataset, I decided to split it in buckets and train the model iteratively.\n",
    "\n",
    "UPDATE:\n",
    "As you see the LB score is decreasing version by version. It was discussed in many topics and kernels that we should not trust this public test set. You can notice from the training kernel that Holdout Kappa score is actually increasing.\n",
    "\n",
    "So, here some questions I ask myself:\n",
    "\n",
    "- **Should we trust our validation score?**\n",
    "- **Is 10% Holdout enough?**\n",
    "- **Should we implement KFold CV?**\n",
    "\n",
    "I'd love to hear what you think :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos2019traineddenset', 'aptos2019-blindness-detection', 'densenet-keras']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "im_size = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1928, 1)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Test Images\n",
    "\n",
    "---\n",
    "\n",
    "Notice that here we can only see the public dataset. Once we submit our results, the kernel will rerun on the overall test set (public + private) that is around 20 GB. That needs to be considered otherwise our kernel may not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset correctly processed\n"
     ]
    }
   ],
   "source": [
    "# Crop function: https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping\n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size/30) ,-4 ,128)\n",
    "    \n",
    "    return img\n",
    "\n",
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, im_size, im_size, 3), dtype=np.uint8)\n",
    "\n",
    "try:\n",
    "    for i, image_id in enumerate(test_df['id_code']):\n",
    "        x_test[i, :, :, :] = preprocess_image(\n",
    "            f'../input/aptos2019-blindness-detection/test_images/{image_id}.png',\n",
    "            desired_size=im_size\n",
    "        )\n",
    "    print('Test dataset correctly processed')\n",
    "except:\n",
    "    print('Test dataset NOT processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - DenseNet 121\n",
    "\n",
    "--- \n",
    "\n",
    "Here we need to replicate the architecture we used during the training phase. The difference is that we are going to load the weights we have previously found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 10, 10, 1024)      7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 7,042,629\n",
      "Trainable params: 6,958,981\n",
      "Non-trainable params: 83,648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "densenet = DenseNet121(\n",
    "    weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5',\n",
    "    include_top=False,\n",
    "    input_shape=(im_size,im_size,3)\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(densenet)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(5, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.load_weights('../input/aptos2019traineddenset/model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id_code  diagnosis\n",
      "0  0005cfc8afb6          2\n",
      "1  003f0afdcd15          3\n",
      "2  006efc72b638          3\n",
      "3  00836aaacf06          2\n",
      "4  009245722fa4          2\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(x_test) > 0.5\n",
    "y_test = y_test.astype(int).sum(axis=1) - 1\n",
    "\n",
    "test_df['diagnosis'] = y_test\n",
    "test_df.to_csv('submission.csv',index=False)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction distribution:\n",
      "2    47.043568\n",
      "3    26.556017\n",
      "0    15.663900\n",
      "1     6.846473\n",
      "4     3.890041\n",
      "Name: diagnosis, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = (test_df.diagnosis.value_counts()/len(test_df))*100\n",
    "print('Prediction distribution:')\n",
    "print(dist)\n",
    "test_df.diagnosis.hist()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
