{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis & Model Tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries needed\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# getting my path C:\\\\Users\\\\username\\\\Desktop\n",
    "# /Users/username/Desktop for Mac\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training set (train.csv), I had this file in C:\\\\Users\\\\username\\\\Desktop\\\\...\n",
    "\n",
    "# Mac\n",
    "\n",
    "path = '/'.join(path.split(\"/\")[:4])\n",
    "df_train = pd.read_csv(path + '/Santander_Customer_Transaction_Prediction/data/train.csv')\n",
    "df_test = pd.read_csv(path + '/Santander_Customer_Transaction_Prediction/data/test.csv')\n",
    "\n",
    "# Windows\n",
    "\n",
    "#path = '\\\\'.join(path.split(\"\\\\\")[:4])\n",
    "#df_train = pd.read_csv(path + '\\\\Santander_Customer_Transaction_Prediction\\\\data\\\\train.csv')\n",
    "#df_test = pd.read_csv(path + '\\\\Santander_Customer_Transaction_Prediction\\\\data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (200000, 202)\n",
      "Test shape: (200000, 201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: \" + str(df_train.shape))\n",
    "print(\"Test shape: \" + str(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the target variable and the features\n",
    "X_train = df_train.loc[:,'var_0':]\n",
    "y_train = df_train.loc[:,'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 200)\n",
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Fake Test Data\n",
    "After a discussion on Kaggle. It seems that the test dataset was created with half real data (used for LB scores) and synthetic data (maybe to increase the diffuculty of the competition). Note that this was one of the most important kernel of the competition, so it is worth looking it :)\n",
    "\n",
    "Here is the kernel: [List of Fake Samples and Public/Private LB split](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_samples_indexes = pd.read_csv('synthetic_samples_indexes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_real = df_test.copy()\n",
    "df_test_real = df_test_real[~df_test_real.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))]\n",
    "X_test = df_test_real.loc[:,'var_0':]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Encoding\n",
    "\n",
    "---\n",
    "\n",
    "As discussed in the [EDA notebook](https://github.com/FedericoRaimondi/me/blob/master/Santander_Customer_Transaction_Prediction/Exploratory_Data_Analysis/Data%20Exploration.ipynb) frequency encoding may help our tree based model to learn also the values occurrences for each variable.\n",
    "\n",
    "I tried both considering only the training set and concatenating train + test.\n",
    "\n",
    "The second path takes significant advantages in terms of performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df):\n",
    "    '''\n",
    "    Function that adds one column for each variable (excluding 'ID_code', 'target')\n",
    "    populated with the value frequencies\n",
    "    '''\n",
    "    for var in [i for i in df.columns if i not in ['ID_code','target']]:\n",
    "        df[var+'_count'] = df.groupby(var)[var].transform('count')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 200)\n"
     ]
    }
   ],
   "source": [
    "X_tot = pd.concat([X_train, X_test])\n",
    "print(X_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 275.81 seconds\n",
      "Shape: \n",
      "(300000, 400)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_tot = get_count(X_tot)\n",
    "end = time.time()\n",
    "print('It took %.2f seconds\\nShape: ' %(end - start))\n",
    "print(X_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = X_tot.iloc[0:200000]\n",
    "X_test_count = X_tot.iloc[200000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build our model\n",
    "\n",
    "But first let's split the train set into train/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "---\n",
    "\n",
    "In this competition LightGBM resulted to be one of the top choices, maybe the best one considering the training time.\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed\n",
    "and efficient with the following advantages:\n",
    "- Faster training speed and higher efficiency\n",
    "- Lower memory usage\n",
    "- Better accuracy\n",
    "- Support of parallel and GPU learning\n",
    "- Capable of handling large-scale data\n",
    "\n",
    "**[Documentation](https://lightgbm.readthedocs.io/en/latest/)**\n",
    "\n",
    "**[Explanation](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8 train, 0.2 dev\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_count, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (160000, 400)\n",
      "\n",
      "y_train shape: (160000,)\n",
      "\n",
      "X_valid shape: (40000, 400)\n",
      "\n",
      "y_valid shape: (40000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: {}\\n'.format(X_train.shape))\n",
    "print('y_train shape: {}\\n'.format(y_train.shape))\n",
    "print('X_valid shape: {}\\n'.format(X_valid.shape))\n",
    "print('y_valid shape: {}'.format(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation 2x if y = 1 , 1x if y = 0\n",
    "def augment(x,y,t=2):\n",
    "    '''\n",
    "    Data Augmentation 2x if y = 1 , 1x if y = 0\n",
    "    '''\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(int(x1.shape[1]/2)):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(int(x1.shape[1]/2)):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Trying Augmentation Only for training set!\n",
    "X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "print('X_tr Augm shape: {}'.format(X_tr.shape))\n",
    "print('y_tr Augm shape: {}'.format(y_tr.shape))\n",
    "\n",
    "end = time.time()\n",
    "print('It took %.2f seconds' %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_tr Augm shape: (336078, 400)\n",
    "- y_tr Augm shape: (336078,)\n",
    "\n",
    "It took 183.11 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = pd.DataFrame(data=X_tr,columns=X_train.columns)\n",
    "y_tr = pd.DataFrame(data=y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the features\n",
    "features = [c for c in X_train.columns if c not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters for Light Gradient Boost\n",
    "lgb_params = {\n",
    "        'bagging_fraction': 0.77,\n",
    "        'bagging_freq': 2,\n",
    "        'lambda_l1': 0.7,\n",
    "        'lambda_l2': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'min_data_in_leaf': 22,\n",
    "        'min_gain_to_split': 0.07,\n",
    "        'min_sum_hessian_in_leaf': 19,\n",
    "        'num_leaves': 20,\n",
    "        'feature_fraction': 1,\n",
    "        'save_binary': True,\n",
    "        'seed': 42,\n",
    "        'feature_fraction_seed': 42,\n",
    "        'bagging_seed': 42,\n",
    "        'drop_seed': 42,\n",
    "        'data_random_seed': 42,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': 'false',\n",
    "        'num_threads': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "# trn_data = lgb.Dataset(X_tr, label=y_tr) # Augmentation\n",
    "val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "# Training\n",
    "clf = lgb.train(lgb_params, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 3000)\n",
    "\n",
    "end = time.time()\n",
    "print('It took %.2f seconds' %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above will take a bit to run...\n",
    "\n",
    "Here are some results I obtained with different approaches:\n",
    "\n",
    "- *CV: 0.900, LB: 0.900*, Model trained with original features\n",
    "- *CV: 0.901, LB: 0.901*, Model trained with original features and augmentation\n",
    "\n",
    "(After competition)\n",
    "\n",
    "- *CV: 0.910, LB: 0.910*, Model trained with frequencies encoding\n",
    "- *CV: 0.916, LB: 0.915*, Model trained with frequencies encoding and augmentation\n",
    "\n",
    "__NOTE__: In the last days of the competition I tried frequency encoding with no luck! That's because 'feature_fraction' parameter... If you don't set it to 1, then the engineered feature may not interact with its original one!\n",
    "Unfortunately, I hadn't this intuition during the competition but I learned something I can use the next time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "train_pred = clf.predict(X_train[features], num_iteration=clf.best_iteration)\n",
    "val_pred = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "predictions = clf.predict(X_test_count[features], num_iteration=clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the ROC AUC scores\n",
    "print(\">> Train score: {:<8.5f}\".format(roc_auc_score(y_train, train_pred)))\n",
    "print(\"\\n>> Valid score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying feature importance\n",
    "\n",
    "importance_df = pd.DataFrame()\n",
    "importance_df[\"feature\"] = features\n",
    "importance_df[\"importance\"] = clf.feature_importance()\n",
    "\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=importance_df)\n",
    "plt.title('LightGBM Features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Faster Approach\n",
    "\n",
    "---\n",
    "\n",
    "Ok the model it's fine, but we need a faster way to make our predictions!\n",
    "\n",
    "Many kernels proposed fascinating approches:\n",
    "- [Felipe Mello's one](https://www.kaggle.com/felipemello/step-by-step-guide-to-the-magic-lb-0-922)\n",
    "- [Chris Deotte's one](https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920)\n",
    "\n",
    "\n",
    "These 2 guys really made an impressive work in the competition and helped many of us in understanding many concepts.\n",
    "\n",
    "Basically, we know that our feature are strongly uncorrelated but we are not sure about their independency...\n",
    "\n",
    "If we manage to create a several models using only one feature and perform the same as before, then we are also sure about independency!\n",
    "\n",
    "In the next code, we are going to build 200 models with only 2 feature: the original one and the related frequency variable (i.e. 'var_0' and 'var_0_count')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 336.93 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "iteration = 120\n",
    "y_hat = np.zeros([int(200000*0.2), 200])\n",
    "test_hat = np.zeros([100000, 200])\n",
    "i = 0\n",
    "for feature in ['var_' + str(x) for x in range(200)]: # loop over all the raw features\n",
    "    feat_choices = [feature, feature + '_count']\n",
    "    trn_data = lgb.Dataset(X_train[feat_choices], y_train)\n",
    "    #trn_data = lgb.Dataset(X_tr[feat_choices], y_tr) # Augmentation\n",
    "    val_data = lgb.Dataset(X_valid[feat_choices], y_valid)\n",
    "    clf = lgb.train(lgb_params, trn_data, iteration, valid_sets=[val_data], verbose_eval=-1)\n",
    "    y_hat[:, i] = clf.predict(X_valid[feat_choices])\n",
    "    test_hat[:, i] = clf.predict(X_test_count[feat_choices])\n",
    "    i += 1\n",
    "    \n",
    "end = time.time()\n",
    "print('It took %.2f seconds' %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Less than 6 min with my pc! That's fast :)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Your CV score is:  0.922140665488566\n"
     ]
    }
   ],
   "source": [
    "val_pred = (y_hat).sum(axis=1)/200\n",
    "predictions = (test_hat).sum(axis=1)/200\n",
    "score = roc_auc_score(y_valid, val_pred)\n",
    "print('>>> Your CV score is: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried both with and without Augmentation. These are the results:\n",
    "\n",
    "- No Augm: *CV: 0.922, LB: 0.92046*\n",
    "- Augm: *CV: 0.921, LB: 0.92038*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Bayesan Optimization\n",
    "\n",
    "---\n",
    "\n",
    "In the Kernel section, I found this [Bayesan Parameter Optimization](https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average) tutorial that helped me tuning the LGBM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    num_leaves,  # int\n",
    "    min_data_in_leaf,  # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf,    # int  \n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. So we make them integer\n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "\n",
    "    param = {\n",
    "        'bagging_fraction': 0.7693,\n",
    "        'bagging_freq': 2,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': max_depth,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'num_leaves': num_leaves,\n",
    "        'feature_fraction': 1,\n",
    "        'save_binary': True,\n",
    "        'seed': 42,\n",
    "        'feature_fraction_seed': 42,\n",
    "        'bagging_seed': 42,\n",
    "        'drop_seed': 42,\n",
    "        'data_random_seed': 42,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': 'false',\n",
    "        'num_threads': 6\n",
    "        }   \n",
    "    \n",
    "    trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    num_round = 5000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [val_data], verbose_eval=5000, early_stopping_rounds = 500)\n",
    "    \n",
    "    predictions = clf.predict(X_valid[features], num_iteration=clf.best_iteration)   \n",
    "    \n",
    "    score = roc_auc_score(y_valid, predictions)\n",
    "    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (10, 25), \n",
    "    'min_data_in_leaf': (15, 40),  \n",
    "    'learning_rate': (0.005, 0.012),\n",
    "    'min_sum_hessian_in_leaf': (15, 20),\n",
    "    'lambda_l1': (0, 5), \n",
    "    'lambda_l2': (0, 5), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(1,10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimizer\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('-' * 100)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic Parameters xD\n",
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this parameters **must be integers:**\n",
    "\n",
    "- num_leaves\n",
    "- min_data_in_leaf\n",
    "- max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold CV\n",
    "\n",
    "---\n",
    "\n",
    "Let's try to split our Training set with KFold cross validation.\n",
    "This should help us to increase a bit our performances and to have more reliable results!\n",
    "\n",
    "I choose 4 Fold, but this could be changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using: ['var_0', 'var_0_count']\n",
      ">>> CV score: 0.54809 \n",
      "Model using: ['var_1', 'var_1_count']\n",
      ">>> CV score: 0.54580 \n",
      "Model using: ['var_2', 'var_2_count']\n",
      ">>> CV score: 0.55087 \n",
      "Model using: ['var_3', 'var_3_count']\n",
      ">>> CV score: 0.50841 \n",
      "Model using: ['var_4', 'var_4_count']\n",
      ">>> CV score: 0.50234 \n",
      "Model using: ['var_5', 'var_5_count']\n",
      ">>> CV score: 0.52743 \n",
      "Model using: ['var_6', 'var_6_count']\n",
      ">>> CV score: 0.55783 \n",
      "Model using: ['var_7', 'var_7_count']\n",
      ">>> CV score: 0.50124 \n",
      "Model using: ['var_8', 'var_8_count']\n",
      ">>> CV score: 0.51733 \n",
      "Model using: ['var_9', 'var_9_count']\n",
      ">>> CV score: 0.54118 \n",
      "Model using: ['var_10', 'var_10_count']\n",
      ">>> CV score: 0.49850 \n",
      "Model using: ['var_11', 'var_11_count']\n",
      ">>> CV score: 0.51800 \n",
      "Model using: ['var_12', 'var_12_count']\n",
      ">>> CV score: 0.55976 \n",
      "Model using: ['var_13', 'var_13_count']\n",
      ">>> CV score: 0.55424 \n",
      "Model using: ['var_14', 'var_14_count']\n",
      ">>> CV score: 0.50525 \n",
      "Model using: ['var_15', 'var_15_count']\n",
      ">>> CV score: 0.51397 \n",
      "Model using: ['var_16', 'var_16_count']\n",
      ">>> CV score: 0.50767 \n",
      "Model using: ['var_17', 'var_17_count']\n",
      ">>> CV score: 0.49677 \n",
      "Model using: ['var_18', 'var_18_count']\n",
      ">>> CV score: 0.53869 \n",
      "Model using: ['var_19', 'var_19_count']\n",
      ">>> CV score: 0.51041 \n",
      "Model using: ['var_20', 'var_20_count']\n",
      ">>> CV score: 0.51746 \n",
      "Model using: ['var_21', 'var_21_count']\n",
      ">>> CV score: 0.55691 \n",
      "Model using: ['var_22', 'var_22_count']\n",
      ">>> CV score: 0.55244 \n",
      "Model using: ['var_23', 'var_23_count']\n",
      ">>> CV score: 0.52179 \n",
      "Model using: ['var_24', 'var_24_count']\n",
      ">>> CV score: 0.52660 \n",
      "Model using: ['var_25', 'var_25_count']\n",
      ">>> CV score: 0.50587 \n",
      "Model using: ['var_26', 'var_26_count']\n",
      ">>> CV score: 0.55814 \n",
      "Model using: ['var_27', 'var_27_count']\n",
      ">>> CV score: 0.49859 \n",
      "Model using: ['var_28', 'var_28_count']\n",
      ">>> CV score: 0.52073 \n",
      "Model using: ['var_29', 'var_29_count']\n",
      ">>> CV score: 0.49670 \n",
      "Model using: ['var_30', 'var_30_count']\n",
      ">>> CV score: 0.49784 \n",
      "Model using: ['var_31', 'var_31_count']\n",
      ">>> CV score: 0.52411 \n",
      "Model using: ['var_32', 'var_32_count']\n",
      ">>> CV score: 0.52733 \n",
      "Model using: ['var_33', 'var_33_count']\n",
      ">>> CV score: 0.53967 \n",
      "Model using: ['var_34', 'var_34_count']\n",
      ">>> CV score: 0.54408 \n",
      "Model using: ['var_35', 'var_35_count']\n",
      ">>> CV score: 0.53568 \n",
      "Model using: ['var_36', 'var_36_count']\n",
      ">>> CV score: 0.53835 \n",
      "Model using: ['var_37', 'var_37_count']\n",
      ">>> CV score: 0.50332 \n",
      "Model using: ['var_38', 'var_38_count']\n",
      ">>> CV score: 0.49953 \n",
      "Model using: ['var_39', 'var_39_count']\n",
      ">>> CV score: 0.49978 \n",
      "Model using: ['var_40', 'var_40_count']\n",
      ">>> CV score: 0.54650 \n",
      "Model using: ['var_41', 'var_41_count']\n",
      ">>> CV score: 0.50277 \n",
      "Model using: ['var_42', 'var_42_count']\n",
      ">>> CV score: 0.50469 \n",
      "Model using: ['var_43', 'var_43_count']\n",
      ">>> CV score: 0.52369 \n",
      "Model using: ['var_44', 'var_44_count']\n",
      ">>> CV score: 0.54417 \n",
      "Model using: ['var_45', 'var_45_count']\n",
      ">>> CV score: 0.51777 \n",
      "Model using: ['var_46', 'var_46_count']\n",
      ">>> CV score: 0.49992 \n",
      "Model using: ['var_47', 'var_47_count']\n",
      ">>> CV score: 0.50475 \n",
      "Model using: ['var_48', 'var_48_count']\n",
      ">>> CV score: 0.53227 \n",
      "Model using: ['var_49', 'var_49_count']\n",
      ">>> CV score: 0.52592 \n",
      "Model using: ['var_50', 'var_50_count']\n",
      ">>> CV score: 0.51169 \n",
      "Model using: ['var_51', 'var_51_count']\n",
      ">>> CV score: 0.52296 \n",
      "Model using: ['var_52', 'var_52_count']\n",
      ">>> CV score: 0.52895 \n",
      "Model using: ['var_53', 'var_53_count']\n",
      ">>> CV score: 0.55352 \n",
      "Model using: ['var_54', 'var_54_count']\n",
      ">>> CV score: 0.51866 \n",
      "Model using: ['var_55', 'var_55_count']\n",
      ">>> CV score: 0.51233 \n",
      "Model using: ['var_56', 'var_56_count']\n",
      ">>> CV score: 0.52629 \n",
      "Model using: ['var_57', 'var_57_count']\n",
      ">>> CV score: 0.51195 \n",
      "Model using: ['var_58', 'var_58_count']\n",
      ">>> CV score: 0.52514 \n",
      "Model using: ['var_59', 'var_59_count']\n",
      ">>> CV score: 0.50615 \n",
      "Model using: ['var_60', 'var_60_count']\n",
      ">>> CV score: 0.50690 \n",
      "Model using: ['var_61', 'var_61_count']\n",
      ">>> CV score: 0.50370 \n",
      "Model using: ['var_62', 'var_62_count']\n",
      ">>> CV score: 0.51036 \n",
      "Model using: ['var_63', 'var_63_count']\n",
      ">>> CV score: 0.51254 \n",
      "Model using: ['var_64', 'var_64_count']\n",
      ">>> CV score: 0.51098 \n",
      "Model using: ['var_65', 'var_65_count']\n",
      ">>> CV score: 0.50607 \n",
      "Model using: ['var_66', 'var_66_count']\n",
      ">>> CV score: 0.51851 \n",
      "Model using: ['var_67', 'var_67_count']\n",
      ">>> CV score: 0.54338 \n",
      "Model using: ['var_68', 'var_68_count']\n",
      ">>> CV score: 0.50813 \n",
      "Model using: ['var_69', 'var_69_count']\n",
      ">>> CV score: 0.50571 \n",
      "Model using: ['var_70', 'var_70_count']\n",
      ">>> CV score: 0.52626 \n",
      "Model using: ['var_71', 'var_71_count']\n",
      ">>> CV score: 0.53003 \n",
      "Model using: ['var_72', 'var_72_count']\n",
      ">>> CV score: 0.50901 \n",
      "Model using: ['var_73', 'var_73_count']\n",
      ">>> CV score: 0.50237 \n",
      "Model using: ['var_74', 'var_74_count']\n",
      ">>> CV score: 0.51842 \n",
      "Model using: ['var_75', 'var_75_count']\n",
      ">>> CV score: 0.53682 \n",
      "Model using: ['var_76', 'var_76_count']\n",
      ">>> CV score: 0.56003 \n",
      "Model using: ['var_77', 'var_77_count']\n",
      ">>> CV score: 0.51365 \n",
      "Model using: ['var_78', 'var_78_count']\n",
      ">>> CV score: 0.54256 \n",
      "Model using: ['var_79', 'var_79_count']\n",
      ">>> CV score: 0.50011 \n",
      "Model using: ['var_80', 'var_80_count']\n",
      ">>> CV score: 0.55819 \n",
      "Model using: ['var_81', 'var_81_count']\n",
      ">>> CV score: 0.57321 \n",
      "Model using: ['var_82', 'var_82_count']\n",
      ">>> CV score: 0.51958 \n",
      "Model using: ['var_83', 'var_83_count']\n",
      ">>> CV score: 0.51634 \n",
      "Model using: ['var_84', 'var_84_count']\n",
      ">>> CV score: 0.50995 \n",
      "Model using: ['var_85', 'var_85_count']\n",
      ">>> CV score: 0.52050 \n",
      "Model using: ['var_86', 'var_86_count']\n",
      ">>> CV score: 0.53826 \n",
      "Model using: ['var_87', 'var_87_count']\n",
      ">>> CV score: 0.53467 \n",
      "Model using: ['var_88', 'var_88_count']\n",
      ">>> CV score: 0.51756 \n",
      "Model using: ['var_89', 'var_89_count']\n",
      ">>> CV score: 0.53284 \n",
      "Model using: ['var_90', 'var_90_count']\n",
      ">>> CV score: 0.52595 \n",
      "Model using: ['var_91', 'var_91_count']\n",
      ">>> CV score: 0.53515 \n",
      "Model using: ['var_92', 'var_92_count']\n",
      ">>> CV score: 0.54268 \n",
      "Model using: ['var_93', 'var_93_count']\n",
      ">>> CV score: 0.52636 \n",
      "Model using: ['var_94', 'var_94_count']\n",
      ">>> CV score: 0.54192 \n",
      "Model using: ['var_95', 'var_95_count']\n",
      ">>> CV score: 0.53339 \n",
      "Model using: ['var_96', 'var_96_count']\n",
      ">>> CV score: 0.49246 \n",
      "Model using: ['var_97', 'var_97_count']\n",
      ">>> CV score: 0.51267 \n",
      "Model using: ['var_98', 'var_98_count']\n",
      ">>> CV score: 0.50025 \n",
      "Model using: ['var_99', 'var_99_count']\n",
      ">>> CV score: 0.55130 \n",
      "Model using: ['var_100', 'var_100_count']\n",
      ">>> CV score: 0.50141 \n",
      "Model using: ['var_101', 'var_101_count']\n",
      ">>> CV score: 0.50838 \n",
      "Model using: ['var_102', 'var_102_count']\n",
      ">>> CV score: 0.51864 \n",
      "Model using: ['var_103', 'var_103_count']\n",
      ">>> CV score: 0.50204 \n",
      "Model using: ['var_104', 'var_104_count']\n",
      ">>> CV score: 0.52388 \n",
      "Model using: ['var_105', 'var_105_count']\n",
      ">>> CV score: 0.51882 \n",
      "Model using: ['var_106', 'var_106_count']\n",
      ">>> CV score: 0.52902 \n",
      "Model using: ['var_107', 'var_107_count']\n",
      ">>> CV score: 0.53704 \n",
      "Model using: ['var_108', 'var_108_count']\n",
      ">>> CV score: 0.54301 \n",
      "Model using: ['var_109', 'var_109_count']\n",
      ">>> CV score: 0.54315 \n",
      "Model using: ['var_110', 'var_110_count']\n",
      ">>> CV score: 0.55784 \n",
      "Model using: ['var_111', 'var_111_count']\n",
      ">>> CV score: 0.52297 \n",
      "Model using: ['var_112', 'var_112_count']\n",
      ">>> CV score: 0.52703 \n",
      "Model using: ['var_113', 'var_113_count']\n",
      ">>> CV score: 0.50793 \n",
      "Model using: ['var_114', 'var_114_count']\n",
      ">>> CV score: 0.52108 \n",
      "Model using: ['var_115', 'var_115_count']\n",
      ">>> CV score: 0.54481 \n",
      "Model using: ['var_116', 'var_116_count']\n",
      ">>> CV score: 0.52157 \n",
      "Model using: ['var_117', 'var_117_count']\n",
      ">>> CV score: 0.50738 \n",
      "Model using: ['var_118', 'var_118_count']\n",
      ">>> CV score: 0.53969 \n",
      "Model using: ['var_119', 'var_119_count']\n",
      ">>> CV score: 0.52482 \n",
      "Model using: ['var_120', 'var_120_count']\n",
      ">>> CV score: 0.51327 \n",
      "Model using: ['var_121', 'var_121_count']\n",
      ">>> CV score: 0.53470 \n",
      "Model using: ['var_122', 'var_122_count']\n",
      ">>> CV score: 0.53933 \n",
      "Model using: ['var_123', 'var_123_count']\n",
      ">>> CV score: 0.54267 \n",
      "Model using: ['var_124', 'var_124_count']\n",
      ">>> CV score: 0.50135 \n",
      "Model using: ['var_125', 'var_125_count']\n",
      ">>> CV score: 0.52020 \n",
      "Model using: ['var_126', 'var_126_count']\n",
      ">>> CV score: 0.49988 \n",
      "Model using: ['var_127', 'var_127_count']\n",
      ">>> CV score: 0.53768 \n",
      "Model using: ['var_128', 'var_128_count']\n",
      ">>> CV score: 0.52211 \n",
      "Model using: ['var_129', 'var_129_count']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> CV score: 0.50336 \n",
      "Model using: ['var_130', 'var_130_count']\n",
      ">>> CV score: 0.52450 \n",
      "Model using: ['var_131', 'var_131_count']\n",
      ">>> CV score: 0.52783 \n",
      "Model using: ['var_132', 'var_132_count']\n",
      ">>> CV score: 0.52029 \n",
      "Model using: ['var_133', 'var_133_count']\n",
      ">>> CV score: 0.54559 \n",
      "Model using: ['var_134', 'var_134_count']\n",
      ">>> CV score: 0.51581 \n",
      "Model using: ['var_135', 'var_135_count']\n",
      ">>> CV score: 0.52331 \n",
      "Model using: ['var_136', 'var_136_count']\n",
      ">>> CV score: 0.49996 \n",
      "Model using: ['var_137', 'var_137_count']\n",
      ">>> CV score: 0.52493 \n",
      "Model using: ['var_138', 'var_138_count']\n",
      ">>> CV score: 0.51759 \n",
      "Model using: ['var_139', 'var_139_count']\n",
      ">>> CV score: 0.57431 \n",
      "Model using: ['var_140', 'var_140_count']\n",
      ">>> CV score: 0.51090 \n",
      "Model using: ['var_141', 'var_141_count']\n",
      ">>> CV score: 0.52950 \n",
      "Model using: ['var_142', 'var_142_count']\n",
      ">>> CV score: 0.51776 \n",
      "Model using: ['var_143', 'var_143_count']\n",
      ">>> CV score: 0.50204 \n",
      "Model using: ['var_144', 'var_144_count']\n",
      ">>> CV score: 0.51797 \n",
      "Model using: ['var_145', 'var_145_count']\n",
      ">>> CV score: 0.52611 \n",
      "Model using: ['var_146', 'var_146_count']\n",
      ">>> CV score: 0.55720 \n",
      "Model using: ['var_147', 'var_147_count']\n",
      ">>> CV score: 0.53941 \n",
      "Model using: ['var_148', 'var_148_count']\n",
      ">>> CV score: 0.54888 \n",
      "Model using: ['var_149', 'var_149_count']\n",
      ">>> CV score: 0.54955 \n",
      "Model using: ['var_150', 'var_150_count']\n",
      ">>> CV score: 0.52422 \n",
      "Model using: ['var_151', 'var_151_count']\n",
      ">>> CV score: 0.52138 \n",
      "Model using: ['var_152', 'var_152_count']\n",
      ">>> CV score: 0.50428 \n",
      "Model using: ['var_153', 'var_153_count']\n",
      ">>> CV score: 0.50241 \n",
      "Model using: ['var_154', 'var_154_count']\n",
      ">>> CV score: 0.54097 \n",
      "Model using: ['var_155', 'var_155_count']\n",
      ">>> CV score: 0.53201 \n",
      "Model using: ['var_156', 'var_156_count']\n",
      ">>> CV score: 0.51623 \n",
      "Model using: ['var_157', 'var_157_count']\n",
      ">>> CV score: 0.52671 \n",
      "Model using: ['var_158', 'var_158_count']\n",
      ">>> CV score: 0.49915 \n",
      "Model using: ['var_159', 'var_159_count']\n",
      ">>> CV score: 0.51193 \n",
      "Model using: ['var_160', 'var_160_count']\n",
      ">>> CV score: 0.50607 \n",
      "Model using: ['var_161', 'var_161_count']\n",
      ">>> CV score: 0.50136 \n",
      "Model using: ['var_162', 'var_162_count']\n",
      ">>> CV score: 0.52917 \n",
      "Model using: ['var_163', 'var_163_count']\n",
      ">>> CV score: 0.53248 \n",
      "Model using: ['var_164', 'var_164_count']\n",
      ">>> CV score: 0.53839 \n",
      "Model using: ['var_165', 'var_165_count']\n",
      ">>> CV score: 0.55221 \n",
      "Model using: ['var_166', 'var_166_count']\n",
      ">>> CV score: 0.55066 \n",
      "Model using: ['var_167', 'var_167_count']\n",
      ">>> CV score: 0.52891 \n",
      "Model using: ['var_168', 'var_168_count']\n",
      ">>> CV score: 0.51235 \n",
      "Model using: ['var_169', 'var_169_count']\n",
      ">>> CV score: 0.54489 \n",
      "Model using: ['var_170', 'var_170_count']\n",
      ">>> CV score: 0.54038 \n",
      "Model using: ['var_171', 'var_171_count']\n",
      ">>> CV score: 0.51218 \n",
      "Model using: ['var_172', 'var_172_count']\n",
      ">>> CV score: 0.53878 \n",
      "Model using: ['var_173', 'var_173_count']\n",
      ">>> CV score: 0.53785 \n",
      "Model using: ['var_174', 'var_174_count']\n",
      ">>> CV score: 0.55327 \n",
      "Model using: ['var_175', 'var_175_count']\n",
      ">>> CV score: 0.52121 \n",
      "Model using: ['var_176', 'var_176_count']\n",
      ">>> CV score: 0.50625 \n",
      "Model using: ['var_177', 'var_177_count']\n",
      ">>> CV score: 0.53365 \n",
      "Model using: ['var_178', 'var_178_count']\n",
      ">>> CV score: 0.51817 \n",
      "Model using: ['var_179', 'var_179_count']\n",
      ">>> CV score: 0.54274 \n",
      "Model using: ['var_180', 'var_180_count']\n",
      ">>> CV score: 0.52640 \n",
      "Model using: ['var_181', 'var_181_count']\n",
      ">>> CV score: 0.51140 \n",
      "Model using: ['var_182', 'var_182_count']\n",
      ">>> CV score: 0.50656 \n",
      "Model using: ['var_183', 'var_183_count']\n",
      ">>> CV score: 0.49695 \n",
      "Model using: ['var_184', 'var_184_count']\n",
      ">>> CV score: 0.54776 \n",
      "Model using: ['var_185', 'var_185_count']\n",
      ">>> CV score: 0.49504 \n",
      "Model using: ['var_186', 'var_186_count']\n",
      ">>> CV score: 0.52836 \n",
      "Model using: ['var_187', 'var_187_count']\n",
      ">>> CV score: 0.51063 \n",
      "Model using: ['var_188', 'var_188_count']\n",
      ">>> CV score: 0.52940 \n",
      "Model using: ['var_189', 'var_189_count']\n",
      ">>> CV score: 0.50737 \n",
      "Model using: ['var_190', 'var_190_count']\n",
      ">>> CV score: 0.55146 \n",
      "Model using: ['var_191', 'var_191_count']\n",
      ">>> CV score: 0.54369 \n",
      "Model using: ['var_192', 'var_192_count']\n",
      ">>> CV score: 0.54315 \n",
      "Model using: ['var_193', 'var_193_count']\n",
      ">>> CV score: 0.51390 \n",
      "Model using: ['var_194', 'var_194_count']\n",
      ">>> CV score: 0.51857 \n",
      "Model using: ['var_195', 'var_195_count']\n",
      ">>> CV score: 0.52538 \n",
      "Model using: ['var_196', 'var_196_count']\n",
      ">>> CV score: 0.51795 \n",
      "Model using: ['var_197', 'var_197_count']\n",
      ">>> CV score: 0.52918 \n",
      "Model using: ['var_198', 'var_198_count']\n",
      ">>> CV score: 0.54874 \n",
      "Model using: ['var_199', 'var_199_count']\n",
      ">>> CV score: 0.52323 \n",
      "It took 2143.74 seconds\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=4, random_state=42)\n",
    "target = df_train['target']\n",
    "y_hat = np.zeros([200000, 200])\n",
    "test_hat = np.zeros([100000, 200])\n",
    "i = 0\n",
    "start = time.time()\n",
    "for feature in ['var_' + str(x) for x in range(200)]: # loop over all features \n",
    "    feat_choices = [feature, feature + '_count']\n",
    "    print('Model using: ' + str(feat_choices))\n",
    "    oof = np.zeros(len(X_train_count))\n",
    "    predictions = np.zeros(len(X_test_count))\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_count[feat_choices].values, target.values)):\n",
    "        trn_data = lgb.Dataset(X_train_count.iloc[trn_idx][feat_choices], label=target.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(X_train_count.iloc[val_idx][feat_choices], label=target.iloc[val_idx])\n",
    "        clf = lgb.train(lgb_params, trn_data, 130, valid_sets = [val_data], verbose_eval=-1)\n",
    "        oof[val_idx] = clf.predict(X_train_count.iloc[val_idx][feat_choices])\n",
    "        predictions += clf.predict(X_test_count[feat_choices]) / folds.n_splits\n",
    "    print(\">>> CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n",
    "    \n",
    "    y_hat[:, i] = oof\n",
    "    test_hat[:, i] = predictions\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "print('It took %.2f seconds' %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, almost *36 min* from my pc. It's quite good!\n",
    "\n",
    "Please note that when I tried KFold CV with all the raw features it took several hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Your CV score is: 0.9209122683537883\n"
     ]
    }
   ],
   "source": [
    "valid_pred = (y_hat).sum(axis=1)/200\n",
    "predictions = (test_hat).sum(axis=1)/200\n",
    "print('>>> Your CV score is:', roc_auc_score(target, valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result I achieved (unfortunately after the end of the competition).\n",
    "Maybe with few more days I could achieve the first step of the magic (LB: 0.908-0.910), but I am grateful for this experience.\n",
    "\n",
    "Kaggle is a great place to learn and practice your data science skills!\n",
    "\n",
    "**4 KFold Results:**\n",
    "- *CV: 0.92091*\n",
    "- *PublicLB: 0.92223*\n",
    "- *PrivateLB: 0.92057*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "---\n",
    "\n",
    "Preparing the submission file!\n",
    "\n",
    "We only need 'ID_codes' and 'pred' columns.\n",
    "\n",
    "__Note__ that I predicted values only for the real test data and I setted the fake ones at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_codes</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.500013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_7</td>\n",
       "      <td>0.499373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_11</td>\n",
       "      <td>0.498615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_15</td>\n",
       "      <td>0.498093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_16</td>\n",
       "      <td>0.500646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_codes      pred\n",
       "3    test_3  0.500013\n",
       "7    test_7  0.499373\n",
       "11  test_11  0.498615\n",
       "15  test_15  0.498093\n",
       "16  test_16  0.500646"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.DataFrame({\"ID_codes\":df_test[~df_test.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))].loc[:,'ID_code']})\n",
    "subm['pred'] = predictions\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_codes = df_test[~df_test.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))].loc[:,'ID_code']\n",
    "submission = pd.DataFrame({\"ID_code\": df_test.ID_code.values})\n",
    "submission['target'] = 0\n",
    "submission.loc[submission['ID_code'].isin(ID_codes), 'target'] = subm['pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.500013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.000000\n",
       "1  test_1  0.000000\n",
       "2  test_2  0.000000\n",
       "3  test_3  0.500013\n",
       "4  test_4  0.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(r'submission.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
